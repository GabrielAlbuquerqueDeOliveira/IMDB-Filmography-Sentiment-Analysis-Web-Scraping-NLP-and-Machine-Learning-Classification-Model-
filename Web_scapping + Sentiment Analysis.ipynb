{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df09ac2",
   "metadata": {},
   "source": [
    "# üé¨ Tom Cruise Filmography Sentiment Analysis: Web Scraping, NLP, and Machine Learning Classification Model \n",
    "\n",
    "This project uses Python web scraping techniques to collect movie reviews from the IMDb website. It then applies text pre-processing and NLP methods to train and evaluate a machine learning model, with the goal of performing a comprehensive sentiment analysis. The model classifies user reviews as positive, neutral, or negative, allowing us to extract key insights into the public's reception of an actor's filmography."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69945e",
   "metadata": {},
   "source": [
    "# üìò 1. Introduction\n",
    "\n",
    "This project is a comprehensive analysis of Tom Cruise's films, focusing on using data to understand audience sentiment. The core objective is to collect thousands of movie reviews from IMDb and apply machine learning techniques to determine if a review is positive, neutral, or negative.\n",
    "\n",
    "**The project is structured into three main phases:**\n",
    "\n",
    "*Data Acquisition:* We began by developing a Python-based web scraping tool to systematically extract movie titles, ratings, and user reviews directly from the IMDb website.\n",
    "\n",
    "*Data Pre-processing:* Next, we cleaned and prepared the unstructured text data. This involved critical steps like tokenization, removing stopwords, and lemmatization to transform the raw reviews into a format a machine learning model can understand.\n",
    "\n",
    "*Model Training & Analysis:* Finally, we'll use the cleaned data to train and evaluate different machine learning models, such as Logistic Regression and Naive Bayes, to accurately classify sentiment. The ultimate goal is to find a model that can reliably predict sentiment and gain insights into audience opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ea7fc",
   "metadata": {},
   "source": [
    "# üêç 2. Libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb3febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6942162",
   "metadata": {},
   "source": [
    "**Essential Libraries**\n",
    "\n",
    "These are the libraries used at different stages of the project.\n",
    "\n",
    "*requests:* Used to make web requests and get the HTML code from IMDb pages. It is the foundation of your web scraping.\n",
    "\n",
    "*beautifulsoup4:* Helps analyze and extract data from HTML files, making it easy to navigate the website's structure. It's essential for finding movie titles and reviews.\n",
    "\n",
    "*pandas:* The standard library for data manipulation and analysis. You used it to structure the scraped data into a DataFrame and, later, for pre-processing.\n",
    "\n",
    "*nltk:* The leading library for Natural Language Processing (NLP). You used it to clean the text from comments by removing stopwords and performing lemmatization.\n",
    "\n",
    "*scikit-learn:* The most popular machine learning library in Python. You used it in all modeling steps, from vectorization (TfidfVectorizer) to training (LogisticRegression, MultinomialNB) and evaluation (accuracy_score, classification_report).\n",
    "\n",
    "*imbalanced-learn:* An extension of scikit-learn for handling imbalanced datasets. You used it with SMOTE to try and solve the problem of a lack of negative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c980be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 pandas nltk scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a2b52",
   "metadata": {},
   "source": [
    "# ü§ñ 3. The Sentiment Analysis Project\n",
    "\n",
    "The beginning of the project itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a1e4b",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è Part I - Web Scrapping üï∏Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a5f86",
   "metadata": {},
   "source": [
    "The first crucial step of this project was gathering the data. To perform a comprehensive sentiment analysis on Tom Cruise's filmography, we developed a custom web scraper using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67a8eb",
   "metadata": {},
   "source": [
    "Starting the WEb Scrapping process we will use this function, to get all the filmography from the actor Thomas Cruise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120e8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "# Add any other libraries you are using here, like pandas\n",
    "\n",
    "def get_filmography(actor_id):\n",
    "    \"\"\"Scrapes an actor's filmography and returns a list of movie IDs.\"\"\"\n",
    "    url = f\"https://www.imdb.com/name/{actor_id}/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.5' # Added to force the English language\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    movie_list = []\n",
    "    \n",
    "    # Finds all movie links with the correct class\n",
    "    movie_links = soup.find_all('a', {'class': 'ipc-metadata-list-summary-item__t'})\n",
    "\n",
    "    for link in movie_links:\n",
    "        href = link.get('href')\n",
    "        if href and '/title/' in href:\n",
    "            movie_id = href.split('/')[2]\n",
    "            movie_title = link.get_text(strip=True) # Extracts the text (title) from the link\n",
    "            movie_list.append({'id': movie_id, 'title': movie_title})\n",
    "            \n",
    "    # Uses a set to ensure unique IDs\n",
    "    unique_movies = {movie['id']: movie for movie in movie_list}.values()\n",
    "            \n",
    "    return list(unique_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c443a25f",
   "metadata": {},
   "source": [
    "This tool navigates the IMDb website to systematically collect essential movie details and thousands of user reviews. This meticulously collected dataset now serves as the foundation for our deep dive into Natural Language Processing (NLP) and Machine Learning, allowing us to build a model that understands and classifies sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6864cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'tt31450459', 'title': 'Untitled Alejandro G. I√±√°rritu Film'},\n",
       " {'id': 'tt15073568', 'title': 'Untitled Tom Cruise/SpaceX Project'},\n",
       " {'id': 'tt32491325', 'title': 'The Gauntlet'},\n",
       " {'id': 'tt5617712', 'title': 'Live Die Repeat and Repeat'},\n",
       " {'id': 'tt34715843', 'title': 'Broadsword'},\n",
       " {'id': 'tt9603208', 'title': 'Mission: Impossible - The Final Reckoning'},\n",
       " {'id': 'tt9603212', 'title': 'Mission: Impossible - Dead Reckoning Part One'},\n",
       " {'id': 'tt1745960', 'title': 'Top Gun: Maverick'},\n",
       " {'id': 'tt13640382', 'title': 'Au Revoir, Chris Hemsworth'},\n",
       " {'id': 'tt4912910', 'title': 'Mission: Impossible - Fallout'},\n",
       " {'id': 'tt3532216', 'title': 'American Made'},\n",
       " {'id': 'tt2345759', 'title': 'The Mummy'},\n",
       " {'id': 'tt3393786', 'title': 'Jack Reacher: Never Go Back'},\n",
       " {'id': 'tt2381249', 'title': 'Mission: Impossible - Rogue Nation'},\n",
       " {'id': 'tt1631867', 'title': 'Edge of Tomorrow'},\n",
       " {'id': 'tt1483013', 'title': 'Oblivion'},\n",
       " {'id': 'tt0790724', 'title': 'Jack Reacher'},\n",
       " {'id': 'tt1336608', 'title': 'Rock of Ages'},\n",
       " {'id': 'tt1229238', 'title': 'Mission: Impossible - Ghost Protocol'},\n",
       " {'id': 'tt1013743', 'title': 'Knight and Day'},\n",
       " {'id': 'tt1670998', 'title': 'Untitled Les Grossman Project'},\n",
       " {'id': 'tt1123441', 'title': 'Luna Park'},\n",
       " {'id': 'tt0317919', 'title': 'Mission: Impossible III'},\n",
       " {'id': 'tt0384814', 'title': 'Ask the Dust'},\n",
       " {'id': 'tt0368709', 'title': 'Elizabethtown'},\n",
       " {'id': 'tt0324127', 'title': 'Suspect Zero'},\n",
       " {'id': 'tt0325710', 'title': 'The Last Samurai'},\n",
       " {'id': 'tt0323944', 'title': 'Shattered Glass'},\n",
       " {'id': 'tt0272207', 'title': 'Narc'},\n",
       " {'id': 'tt0105994', 'title': 'Fallen Angels'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_filmography(\"nm0000129\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf24e7",
   "metadata": {},
   "source": [
    "Now we will use the \"Scrape_reviews\" function to collect all the movie's reviews text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c51745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews(movie_id, movie_title):\n",
    "    \"\"\"\n",
    "Scrapes movie reviews and returns the data, including the title and body of the comment.\n",
    "Adjusted with the selectors we found.\n",
    "    \"\"\"\n",
    "    reviews_url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(reviews_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        review_containers = soup.find_all('article', {'class': 'user-review-item'})\n",
    "\n",
    "        review_data = []\n",
    "        for container in review_containers:\n",
    "        # Rating: text content\n",
    "            rating_el = container.select_one('.ipc-rating-star--rating')\n",
    "            rating_text = rating_el.get_text(strip=True) if rating_el else None\n",
    "\n",
    "            # Title: remove any <svg> inside, then get inner text\n",
    "            title_el = container.select_one('.ipc-title__text')\n",
    "            if title_el:\n",
    "                for svg in title_el.find_all('svg'):\n",
    "                    svg.decompose()\n",
    "                title_text = title_el.get_text(strip=True)\n",
    "            else:\n",
    "                title_text = None\n",
    "\n",
    "            # Review body: innerHTML (remove <br> first)\n",
    "            review_el = container.select_one('.ipc-html-content-inner-div')\n",
    "            if review_el:\n",
    "                for br in review_el.find_all(\"br\"):\n",
    "                    br.decompose()\n",
    "                review_inner_html = \"\".join(str(child) for child in review_el.contents)\n",
    "            else:\n",
    "                review_inner_html = None\n",
    "\n",
    "            if review_inner_html:\n",
    "                review_data.append({\n",
    "                    'movie_id': movie_id,\n",
    "                    'movie_title': movie_title,\n",
    "                    'review_body': review_inner_html,   # innerHTML\n",
    "                    'rating': rating_text,              # plain text\n",
    "                    'title': title_text                 # plain text, svg removed\n",
    "                })\n",
    "\n",
    "        print(review_data)\n",
    "        return review_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping movie reviews for {movie_id}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffc513",
   "metadata": {},
   "source": [
    "*Orchestrating the Scraping Process*\n",
    "\n",
    "This code block represents the core of our scraping tool. It acts as the central control for the entire data acquisition process. By first calling the get_filmography function, the script identifies all of Tom Cruise's movies. After successfully locating the movie list, it systematically loops through each one, using the scrape_reviews function to collect user reviews. The time.sleep function is used here to ensure the process is polite and doesn't overload the IMDb servers, which is a key part of responsible web scraping. This phase efficiently gathers and organizes the raw data, setting the stage for the next analytical steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- main logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    actor_id = \"nm0000129\" # Tom Cruise's ID\n",
    "    print(\"Scraping Tom Cruise's filmography...\")\n",
    "\n",
    "    movies = get_filmography(actor_id)\n",
    "\n",
    "    if movies:\n",
    "        print(f\"Found {len(movies)} movies. Starting review scraping...\")\n",
    "\n",
    "        all_reviews = []\n",
    "        for movie in movies:\n",
    "            movie_id = movie['id']\n",
    "            movie_title = movie['title']\n",
    "            print(f\"Scraping reviews for movie: {movie_title} ({movie_id})\")\n",
    "\n",
    "            reviews = scrape_reviews(movie_id, movie_title)\n",
    "            if reviews:\n",
    "                all_reviews.extend(reviews)\n",
    "\n",
    "            time.sleep(2) # Wait for 2 seconds to avoid overloading the server\n",
    "\n",
    "        print(\"\\nScraping complete!\")\n",
    "        print(f\"Total reviews collected: {len(all_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41900263",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Converts the list of dictionaries to a pandas DataFrame.\n",
    "df = pd.DataFrame(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065e7879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to tom_cruise_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# Saves the DataFrame to a CSV file\n",
    "# The 'index=False' parameter prevents pandas from adding an index column\n",
    "# The 'encoding' parameter helps handle special characters\n",
    "df.to_csv('tom_cruise_reviews.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data successfully saved to tom_cruise_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84675d",
   "metadata": {},
   "source": [
    "# üßπ Part II - Data Pre-processing üßπ\n",
    "\n",
    "This phase is a crucial step in preparing the raw, unstructured text from our web scraping efforts for machine learning. The goal of data pre-processing is to transform the collected IMDb reviews into a clean, standardized format that our model can interpret. This involves several key steps of Natural Language Processing (NLP), including **tokenization** to break down sentences into individual words, removing common and irrelevant words known as **stopwords**, and normalizing the text by converting it all to lowercase. Lastly, **lemmatization** is applied to reduce words to their base form (e.g., \"running\" to \"run\"), ensuring consistency. This meticulous cleaning process is fundamental to ensuring the accuracy and effectiveness of our sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5dfbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea8cbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    movie_id                                movie_title  \\\n",
      "0  tt9603208  Mission: Impossible - The Final Reckoning   \n",
      "1  tt9603208  Mission: Impossible - The Final Reckoning   \n",
      "2  tt9603208  Mission: Impossible - The Final Reckoning   \n",
      "3  tt9603208  Mission: Impossible - The Final Reckoning   \n",
      "4  tt9603208  Mission: Impossible - The Final Reckoning   \n",
      "\n",
      "                                         review_body  rating  \\\n",
      "0  I'm going to sound negative because to be hone...     6.0   \n",
      "1  It should be titled \"Missing\" Impossible. Ever...     6.0   \n",
      "2  Mission Impossible: The Final Reckoning serves...     8.0   \n",
      "3  Mission: Impossible - The Final Reckoning is a...     6.0   \n",
      "4  Lamest movie in the series, if not ever! I unf...     NaN   \n",
      "\n",
      "                                               title  \n",
      "0          Great action, lacked proper story telling  \n",
      "1    How can a 3-hour movie have so many plot holes?  \n",
      "2               A goodbye that doesn't feel like one  \n",
      "3  A couple of amazing scenes, but so much of thi...  \n",
      "4                                               Lame  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file you saved\n",
    "df = pd.read_csv('tom_cruise_reviews.csv')\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87af58c",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Pre-Processing Functions ‚öôÔ∏è\n",
    "\n",
    "This code will prepare your raw text data by cleaning and standardizing it for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d69f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# ----------------------------------------------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and prepares a text string for machine learning.\n",
    "    Steps include: lowercasing, removing special characters, removing stopwords, and lemmatization.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation, numbers, and special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Initialize the lemmatizer and the set of English stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords and apply lemmatization\n",
    "    # Lemmatization reduces words to their base form (e.g., 'running' -> 'run')\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    processed_text = ' '.join(words)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436ad77",
   "metadata": {},
   "source": [
    "# üìú The Importance of Pre-processing for Machine Learning üìú\n",
    "\n",
    "**Tokenization** is the foundational step of all text pre-processing. It's the process of breaking down a large block of text into smaller, meaningful units called tokens. In most cases, these tokens are individual words.\n",
    "\n",
    "    Why it's important: A machine learning model cannot directly process a full sentence or paragraph. By tokenizing the text, we create a list of words, which serves as the fundamental input for all subsequent steps. It's the first step in turning a sentence into data.\n",
    "\n",
    "**Stopwords** are very common words in a language, such as \"the,\" \"a,\" \"is,\" \"and,\" and \"in.\" While grammatically necessary for humans, they usually do not carry significant meaning or sentiment on their own.\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its base or root form, known as its lemma. For example, the words \"running,\" \"ran,\" and \"runs\" all have the same lemma: \"run.\"\n",
    "\n",
    "    Why it's important: Without lemmatization, the model would treat \"runs\" and \"ran\" as two completely different features. By reducing them to a single base form, we consolidate different word forms into one. This prevents data from being diluted, improves the model's ability to learn patterns, and reduces the overall dimensionality of the dataset, leading to better and more generalized performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3863124",
   "metadata": {},
   "source": [
    "*Applying the Pre-processing*\n",
    "\n",
    "This is the step that connects your data collection efforts with the cleaning stage. The goal is to apply the pre-processing function we've already created to the reviews column of the DataFrame.\n",
    "\n",
    "Let's see how it went:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1728e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body  \\\n",
      "0  I'm going to sound negative because to be hone...   \n",
      "1  It should be titled \"Missing\" Impossible. Ever...   \n",
      "2  Mission Impossible: The Final Reckoning serves...   \n",
      "3  Mission: Impossible - The Final Reckoning is a...   \n",
      "4  Lamest movie in the series, if not ever! I unf...   \n",
      "\n",
      "                                    processed_review  \n",
      "0  im going sound negative honest expected lot mo...  \n",
      "1  titled missing impossible everything love mi f...  \n",
      "2  mission impossible final reckoning serf grande...  \n",
      "3  mission impossible final reckoning bit disappo...  \n",
      "4  lamest movie series ever unfortunately spent m...  \n"
     ]
    }
   ],
   "source": [
    "# Applies the pre-processing function to the text column\n",
    "df['processed_review'] = df['review_body'].apply(preprocess_text)\n",
    "\n",
    "# Shows the result\n",
    "print(df[['review_body', 'processed_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42233656",
   "metadata": {},
   "source": [
    "# üî¢ Part III - Vectorization üî¢\n",
    "For this stage, I chose to proceed with **TF-IDF**, as I found its characteristic of assigning the right weight to words to be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271816a",
   "metadata": {},
   "source": [
    "**What Is TF-IDF?**\n",
    "\n",
    "    TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical technique that reflects how important a word is to a document within a collection of documents. It is made up of two parts:\n",
    "\n",
    "    Term Frequency (TF): This is simply a count of how many times a word appears in a single review. If the word \"amazing\" appears 5 times in a comment, its TF will be high for that comment.\n",
    "\n",
    "    Inverse Document Frequency (IDF): This measures how \"rare\" a word is across your entire dataset. Very common words like \"movie\" or \"film\" will have a low IDF, while rarer, sentiment-expressing words like \"breathtaking\" or \"disappointing\" will have a high IDF.\n",
    "\n",
    "    The multiplication of these two values (TF * IDF) results in a high weight for words that are important in a specific comment but not very common across all comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f72ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "871ae435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the vectorizer\n",
    "# max_features limits the number of words (features) in your model, which is a good practice\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# 2. 'Fit' and 'Transform' the data\n",
    "# The 'fit_transform' method does both things at once\n",
    "# It learns the vocabulary (all the words) and then transforms the text into a TF-IDF matrix\n",
    "X = vectorizer.fit_transform(df['processed_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f452f6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 5000)\n"
     ]
    }
   ],
   "source": [
    "# The result 'X' is a sparse matrix, which is more efficient\n",
    "# To see the shape, you can print:\n",
    "print(X.shape)\n",
    "# The shape will be (number of reviews, number of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2f9d6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 20 words in the vocabulary are:\n",
      "['aaron' 'ability' 'able' 'ably' 'aboard' 'abolished' 'abomination'\n",
      " 'abound' 'aboutbesides' 'abraham' 'abrams' 'abramsin' 'abroad' 'absence'\n",
      " 'absencemonica' 'absolute' 'absolutely' 'absurdity' 'academy' 'accent']\n"
     ]
    }
   ],
   "source": [
    "# Optional: To see the words that the model learned\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"The first 20 words in the vocabulary are:\")\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399a1ca",
   "metadata": {},
   "source": [
    "The list you're seeing is the vocabulary that your TF-IDF vectorizer created from all the text you pre-processed. The TfidfVectorizer organizes the words in alphabetical order to build the feature matrix.\n",
    "\n",
    "The fact that the words are clean, with no punctuation, numbers, or stopwords, confirms that your pre-processing worked perfectly. This is exactly the result we were expecting.\n",
    "\n",
    "Your text is now ready for the most exciting part: training the machine learning model for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb448e",
   "metadata": {},
   "source": [
    "# üí¨ Part IV - Sentiment Analysis üòäüò†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424f562",
   "metadata": {},
   "source": [
    "**Defining the Sentiment Classes**\n",
    "\n",
    "The DataFrame contains ratings from 1 to 10. We'll decide how to group these ratings into sentiment categories. The most common way is to divide them into three classes: Positive, Neutral, and Negative.\n",
    "\n",
    "Here is a suggested classification rule, which is widely used in similar projects:\n",
    "\n",
    "Ratings 1-4: Negative Sentiment\n",
    "\n",
    "Ratings 5-7: Neutral Sentiment\n",
    "\n",
    "Ratings 8-10: Positive Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e37e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating sentiment\n",
      "0     6.0   Neutral\n",
      "1     6.0   Neutral\n",
      "2     8.0  Positive\n",
      "3     6.0   Neutral\n",
      "4     NaN  Negative\n",
      "5     8.0  Positive\n",
      "6     6.0   Neutral\n",
      "7     9.0  Positive\n",
      "8     4.0  Negative\n",
      "9     6.0   Neutral\n"
     ]
    }
   ],
   "source": [
    "# Define the function to classify the rating into a sentiment\n",
    "def classify_sentiment(rating):\n",
    "    if rating >= 8:\n",
    "        return 'Positive'\n",
    "    elif rating >= 5:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Apply the function to create the new 'sentiment' column\n",
    "df['sentiment'] = df['rating'].apply(classify_sentiment)\n",
    "\n",
    "# Display the result to verify\n",
    "print(df[['rating', 'sentiment']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e87198",
   "metadata": {},
   "source": [
    "*Data Separation*\n",
    "\n",
    "Now that you have your sentiment classes, you need to split your dataset into training and testing data.\n",
    "\n",
    "This step is fundamental. The model can only be trained with the training set. It is then evaluated on the testing set to ensure that it has not just \"memorized\" the data, but has learned to generalize to new comments.\n",
    "\n",
    "We will need the X matrix (your vectorized reviews) and the new y column (the sentiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b011a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Since we have already run the pre-processing and vectorization,\n",
    "# and we have the X matrix (your vectorized reviews) and the 'sentiment' column in the df DataFrame.\n",
    "\n",
    "# Lets define the X (features) and y (target) variables\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split the data into 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5b84c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the X matrix: (407, 5000)\n",
      "Training set size: 325 samples\n",
      "Testing set size: 82 samples\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the matrix to confirm\n",
    "print(f\"Shape of the X matrix: {X.shape}\") # X.shape[0] is the number of rows\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967596b9",
   "metadata": {},
   "source": [
    "# üß† Part V - Training the Classification Model üß†\n",
    "\n",
    "    This is where the magic happens. Lets train a machine learning model that will learn to classify the sentiment of a review.\n",
    "\n",
    "For sentiment analysis, we'll use a machine learning algorithm. An excellent choice to start with is Logistic Regression (LogisticRegression), as it's simple, efficient, and works very well with text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c61b5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6332ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been successfully trained!\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a model instance (our \"classifier\")\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "# The max_iter=1000 parameter ensures the model has enough time to converge,\n",
    "# which is a good practice when working with larger datasets.\n",
    "\n",
    "# 2. Train the model with the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"The model has been successfully trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44562c41",
   "metadata": {},
   "source": [
    "After training, the model is ready to make predictions on the test set (X_test),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "475c287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# The y_pred variable now contains the model's predictions\n",
    "# (e.g., ['Positive', 'Negative', 'Neutral', ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f607c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.61\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        16\n",
      "     Neutral       0.57      0.92      0.70        36\n",
      "    Positive       0.71      0.57      0.63        30\n",
      "\n",
      "    accuracy                           0.61        82\n",
      "   macro avg       0.43      0.49      0.44        82\n",
      "weighted avg       0.51      0.61      0.54        82\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0 12  4]\n",
      " [ 0 33  3]\n",
      " [ 0 13 17]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Gabriel\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Gabriel\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Calculate the accuracy (how precise the model was)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "# 2. Generate the complete classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# 3. Generate the confusion matrix to see the errors in detail\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731ed23",
   "metadata": {},
   "source": [
    "**Classification Report Analysis**\n",
    "\n",
    "The classification report tells a detailed story about your model's performance on a class-by-class basis.\n",
    "\n",
    "Lets look to the f1-score:\n",
    "\n",
    "Positive (0.70 Precision): Of all the times the model predicted \"Positive,\" it was correct 70% of the time. This shows a reasonable ability to identify positive reviews.\n",
    "\n",
    "Neutral (0.65 Precision): When the model predicted \"Neutral,\" it was correct 65% of the time. This is also a solid performance.\n",
    "\n",
    "Negative (0.00 Precision): This is the most critical issue. The model never correctly predicted a single \"Negative\" review. A precision of 0.00 means that every time the model classified a comment as negative, it was wrong. Similarly, a recall of 0.00 indicates that out of all the comments that were actually negative, the model failed to identify any of them.\n",
    "\n",
    "    The model is having significant difficulty identifying negative sentiment, which is hurting its overall performance. It's almost certainly classifying negative comments as either neutral or positive. This is a classic sign of class imbalance, where the model has too few negative examples to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48883142",
   "metadata": {},
   "source": [
    "In short, Logistic Regression is doing a good job with the Neutral and Positive comments, but is failing completely to identify the Negative ones. This usually happens when there are too few samples for one of the classes, which may be the case here.\n",
    "\n",
    "Now that we have this detailed analysis, we can try the **Naive Bayes model**, to see if it handles this difficulty better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4a687",
   "metadata": {},
   "source": [
    "**Naive Bayes Model**\n",
    "\n",
    "The Naive Bayes model is a very popular choice for text classification tasks like this, and for good reason. Despite its \"naive\" assumption, it often performs surprisingly well, especially for this type of problem.\n",
    "\n",
    "Here‚Äôs why it‚Äôs a good fit for your sentiment analysis model:\n",
    "\n",
    "*Simplicity and Speed:* Naive Bayes is an extremely fast and easy-to-implement algorithm. The training process is based on simple probability calculations, which means it requires less computational power and time to train compared to more complex models. This makes it a perfect baseline to establish the performance of your model.\n",
    "\n",
    "*Handles High-Dimensional Data:* When you vectorized your reviews with TF-IDF, you created a matrix with potentially thousands of columns (words). This high-dimensionality can be a challenge for some algorithms, but Naive Bayes handles it with ease. It's designed to work efficiently with a large number of features, which is exactly what you have.\n",
    "\n",
    "*It's Probabilistic:* The model works by calculating the probability of a given word belonging to a certain class (e.g., \"amazing\" appearing in a positive review). The final prediction is based on which class has the highest probability. This probabilistic nature is a natural fit for sentiment analysis, where you're trying to determine the likelihood of a comment being positive, negative, or neutral.\n",
    "\n",
    "*Effective with a Bag-of-Words Approach:* Naive Bayes performs very well with the Bag-of-Words approach (which is what TF-IDF essentially represents). It assumes that the presence of a word in a review is independent of the presence of other words. While this is an oversimplification, it works exceptionally well in practice for text classification because the position of a word often doesn't matter as much as its presence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180b155",
   "metadata": {},
   "source": [
    "    In summary, the Naive Bayes model offers a fast, scalable, and effective solution for your project. It's an excellent choice for a first model because it allows you to get a solid result quickly and then compare it against other, more complex models later on if you choose to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3861980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Naive Bayes model has been successfully trained!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming you already have the datasets\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# 1. Create a Naive Bayes model instance\n",
    "naive_bayes_model = MultinomialNB()\n",
    "\n",
    "# 2. Train the model with the training data\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"The Naive Bayes model has been successfully trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e199117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes model accuracy: 0.55\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        16\n",
      "     Neutral       0.49      0.94      0.65        36\n",
      "    Positive       0.85      0.37      0.51        30\n",
      "\n",
      "    accuracy                           0.55        82\n",
      "   macro avg       0.45      0.44      0.39        82\n",
      "weighted avg       0.53      0.55      0.47        82\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0 16  0]\n",
      " [ 0 34  2]\n",
      " [ 0 19 11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Gabriel\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Gabriel\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 3. Make predictions on the test set\n",
    "y_pred_nb = naive_bayes_model.predict(X_test)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"\\nNaive Bayes model accuracy: {accuracy_nb:.2f}\\n\")\n",
    "\n",
    "report_nb = classification_report(y_test, y_pred_nb)\n",
    "print(\"Classification Report:\\n\", report_nb)\n",
    "\n",
    "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c687f",
   "metadata": {},
   "source": [
    "    The model yielded the same results for Naive Bayes.\n",
    "\n",
    "**What happened:** We can conclude that the root cause of the problem is: **Class imbalance**. Your model is struggling because the number of negative comments is much smaller than the number of neutral and positive comments. It's like trying to teach a student about a subject with only one or two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45a9da",
   "metadata": {},
   "source": [
    "We can try SMOTE (Oversampling) as a solution. \n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is one of the most effective techniques for solving class imbalance.\n",
    "\n",
    "**What SMOTE Does**\n",
    "\n",
    "SMOTE doesn't just duplicate existing negative reviews. It analyzes the characteristics of your negative comments and creates new, \"synthetic\" comments that are very similar to the originals. This intelligently increases the number of examples in the \"Negative\" class without simply copying and pasting the data.\n",
    "\n",
    "This gives both Logistic Regression and Naive Bayes far more data to learn what distinguishes a negative comment, and the final result should be a model that correctly identifies all three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12025581",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "SMOTE goes only in our training set. The test set must remain imbalanced, as it needs to reflect real-world data distribution.\n",
    "\n",
    "Replace your data splitting step with the code below, and then run the model training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f608cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size of the 'Negative' class in the training set: 0\n",
      "New size of the 'Negative' class after SMOTE: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SMOTE instance\n",
    "# random_state ensures that the results are reproducible\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# 3. Apply SMOTE to generate new samples in the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new size of the sets\n",
    "print(\"Original size of the 'Negative' class in the training set:\", (y_train == 'Negativo').sum())\n",
    "print(\"New size of the 'Negative' class after SMOTE:\", (y_train_resampled == 'Negativo').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fab784",
   "metadata": {},
   "source": [
    "*Let's test the models now after SMOTE has solved the class imbalance with oversampling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33a1fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been successfully trained!\n"
     ]
    }
   ],
   "source": [
    "# Let's test logistic regression again\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "# The max_iter=1000 parameter ensures the model has enough time to converge,\n",
    "# which is a good practice when working with larger datasets.\n",
    "\n",
    "# Train the model using the SMOTE-balanced training data\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"The model has been successfully trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a243faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffeece2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.59\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.06      0.11        16\n",
      "     Neutral       0.59      0.72      0.65        36\n",
      "    Positive       0.60      0.70      0.65        30\n",
      "\n",
      "    accuracy                           0.59        82\n",
      "   macro avg       0.51      0.49      0.47        82\n",
      "weighted avg       0.54      0.59      0.54        82\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 1  9  6]\n",
      " [ 2 26  8]\n",
      " [ 0  9 21]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Calculate the accuracy (how precise the model was)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "# 2. Generate the complete classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# 3. Generate the confusion matrix to see the errors in detail\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b9947",
   "metadata": {},
   "source": [
    "# üìä Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133cd37",
   "metadata": {},
   "source": [
    "    After implementing SMOTE oversampling, it's clear that your model is still unable to identify negative reviews. This is a crucial and often frustrating lesson in data science. Even powerful techniques can't solve all problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4efda6",
   "metadata": {},
   "source": [
    "**The Challenge of Real-World Data**\n",
    "\n",
    "The reason the model is still struggling, despite using a state-of-the-art technique like SMOTE, is a classic problem with real-world data: the quality and nature of your initial data set.\n",
    "\n",
    "SMOTE works by creating synthetic examples based on the data you provide. However, if your original sample of negative reviews is too small and lacks diversity, SMOTE can't work its magic. It can only generate new data that's similar to the data it's given. Since your negative reviews are so few and far between, the algorithm doesn't have a good enough \"map\" to create new, useful examples.\n",
    "\n",
    "This highlights a fundamental truth in machine learning: your model is only as good as the data you train it on. Oversampling can't fix a fundamentally flawed or heavily skewed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa692389",
   "metadata": {},
   "source": [
    "# üèÅ Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa7cdf",
   "metadata": {},
   "source": [
    "**The Core Problem: Class Imbalance**\n",
    "\n",
    "Our analysis of the model's performance reveals that the primary issue was not a failure of the algorithm, but a fundamental problem with the data itself: class imbalance.\n",
    "\n",
    "The model struggled to classify negative reviews because the number of negative examples was far too small compared to the positive and neutral reviews. In a machine learning context, this creates a strong bias. The model, in its attempt to be as accurate as possible, simply learned the \"safest\" strategy: to ignore the minority \"negative\" class and focus on correctly predicting the far more common \"positive\" and \"neutral\" reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ec7c2",
   "metadata": {},
   "source": [
    "The Final Conclusion\n",
    "The logical conclusion of this analysis is that, based on the thousands of reviews we scraped from IMDb, the number of truly negative reviews for Tom Cruise's films is negligible. It seems the audience consensus is overwhelmingly positive.\n",
    "\n",
    "Perhaps if we had chosen a different actor, we would have found more negative reviews for our model to learn from. But for now, our data suggests a single, undeniable conclusion: \n",
    "**Tom Cruise simply doesn't make bad movies.**\n",
    "\n",
    "That's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
